{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ducpv/miniconda3/envs/machinelearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/ducpv/miniconda3/envs/machinelearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reference_test.txt\", \"r\") as r:\n",
    "    ref = r.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"candidate_test.txt\", \"r\") as c:\n",
    "    can = c.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "referenceText = [sentence.replace(\"\\n\", \"\") for sentence in ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lớn lên ở bắc triều tiên , hyeonseo lee đã từng nghĩ rằng đất nước mình là \" tốt nhất trên thế giới \" . chỉ sau khi chứng kiến nạn đói vào những năm 90 cô mới bắt đầu có những suy nghĩ khác. cô đã dời bỏ bắc triều tiên khi 14 tuổi và bắt đầu sống che dấu danh tính như một người tị nạn tại trung quốc. đây là câu chuyện đau lòng về cuộc hành trình để tồn tại đầy hy vọng của hyeonseo lee , và cũng là lời nhắn nhủ từ những người luôn phải sống với nỗi lo sợ thường trực , kể cả khi biên giới đã ở rất xa .',\n",
       " 'hyeonseo lee : hành trình chạy trốn khỏi bắc triều tiên',\n",
       " 'khi tôi còn nhỏ , tôi nghĩ rằng bắctriều tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài \" chúng ta chẳng có gì phải ghen tị. \"',\n",
       " 'tôi đã rất tự hào về đất nước tôi .',\n",
       " 'ở trường , chúng tôi dành rất nhiều thời gian để học về cuộc đời của chủ tịch kim ii - sung , nhưng lại không học nhiều về thế giới bên ngoài , ngoại trừ việc hoa kỳ , hàn quốc và nhật bản là kẻ thù của chúng tôi .',\n",
       " 'mặc dù tôi đã từng tự hỏi không biết thế giới bên ngoài kia như thế nào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộc đời ở bắctriều tiên , cho tới khi tất cả mọi thứ đột nhiên thay đổi .',\n",
       " 'khi tôi lên 7 , tôi chứng kiến cảnh người ta xử bắn công khai lần đầu tiên trong đời , nhưng tôi vẫn nghĩ cuộc sống của mình ở đây là hoàn toàn bình thường .',\n",
       " 'gia đình của tôi không nghèo , và bản thân tôi thì chưa từng phải chịu đói .',\n",
       " 'nhưng vào một ngày của năm 1995 , mẹ tôi mang về nhà một lá thư từ một người chị em cùng chỗ làm với mẹ .',\n",
       " 'trong đó có viết : khi chị đọc được những dòng này thì cả gia đình 5 người của em đã không còn trên cõi đời này nữa , bởi vì cả nhà em đã không có gì để ăn trong hai tuần .']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "referenceText[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateText = [sentence.replace(\"\\n\", \"\") for sentence in can]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['khi một đứa trẻ trưởng thành ở bắc triều tiên đã nghĩ rằng đất nước của mình là \" những người tốt nhất trên hành tinh này. \" nó không phải cho tới khi nạn đói của sự bùng nổ mà cô ấy bắt đầu tự hỏi cô ấy tiến hành đất nước vào 14 người , để bắt đầu một cuộc sống ở trung quốc , như một người tị nạn của trung quốc .',\n",
       " 'edward lee : từ bắc triều tiên',\n",
       " 'khi tôi còn nhỏ , tôi nghĩ đất nước mình là người giỏi nhất trên hành tinh , và tôi lớn lên hát hát một bài hát có tên là \" không có gì cho chúa \"',\n",
       " 'và tôi rất tự hào .',\n",
       " 'tại trường học , chúng tôi đã dành rất nhiều thời gian nghiên cứu lịch sử , nhưng chúng ta chưa bao giờ biết về thế giới bên ngoài , ngoại trừ nước mỹ , nam hàn , nhật bản là những kẻ thù .',\n",
       " 'mặc dù tôi thường tự hỏi về thế giới bên ngoài , tôi nghĩ rằng tôi sẽ dành cả đời mình ở bắc triều tiên , cho đến khi mọi thứ đột ngột thay đổi .',\n",
       " 'khi tôi 7 tuổi , tôi đã thấy biểu đồ công cộng đầu tiên , nhưng tôi nghĩ cuộc sống của tôi ở bắc triều tiên là bình thường .',\n",
       " 'gia đình tôi không nghèo , và tôi , tôi chưa bao giờ gặp nạn đói .',\n",
       " 'nhưng một ngày , năm 1995 , mẹ tôi đã mang về nhà một lá thư từ chị gái .',\n",
       " 'nó đọc , \" khi bạn đọc điều này , tất cả 5 thành viên gia đình sẽ không tồn tại trong thế giới này , bởi vì chúng ta chưa ăn mừng trong 2 tuần qua .']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidateText[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(referenceText) == len(candidateText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate translation\n",
    "def evaluate_model(candidate, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for can, raw in tqdm(zip(candidate, raw_dataset)):                        \n",
    "        actual.append([raw.split()])\n",
    "        predicted.append(can.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1304it [00:00, 126940.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.558451\n",
      "BLEU-2: 0.416025\n",
      "BLEU-3: 0.353167\n",
      "BLEU-4: 0.243667\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(candidateText, referenceText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://www.nltk.org/api/nltk.translate.html#nltk.translate.chrf_score.sentence_chrf\n",
    "#### https://www.intechopen.com/books/recent-trends-in-computational-intelligence/machine-translation-and-the-evaluation-of-its-quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.chrf_score import sentence_chrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate translation\n",
    "def evaluate_sentence(candidate, raw_dataset):\n",
    "    crf_scores = list()\n",
    "    actual, predicted = list(), list()\n",
    "    for can, raw in tqdm(zip(candidate, raw_dataset)):  \n",
    "        crf_scores.append(sentence_chrf(raw, can))\n",
    "    return np.mean(crf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1304it [00:01, 702.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42719599401174774"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_sentence(candidateText, referenceText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = referenceText + candidateText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sen.split(\" \") for sen in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents, vector_size=128, min_count=1, epochs = 50, dm=1, dm_concat=0, dm_mean=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import softcossim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "referenceSplit = [text.split(\" \") for text in referenceText]\n",
    "candidateSplit = [text.split(\" \") for text in candidateText]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaryRef = corpora.Dictionary(referenceSplit)\n",
    "dictionaryCan = corpora.Dictionary(candidateSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"doc2vecGensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5638791024684906\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "#d2v_model = doc2vec.Doc2Vec.load(model_file)\n",
    "\n",
    "#fisrt_text = '..'\n",
    "#second_text = '..'\n",
    "vec1 = model.infer_vector(referenceSplit[0])\n",
    "vec2 = model.infer_vector(candidateSplit[0])\n",
    "\n",
    "similairty = spatial.distance.cosine(vec1, vec2)\n",
    "print(similairty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1304it [00:11, 117.32it/s]\n"
     ]
    }
   ],
   "source": [
    "cosineScore = list()\n",
    "for ref, can in tqdm(zip(referenceSplit, candidateSplit)):\n",
    "    vec1 = model.infer_vector(ref)\n",
    "    vec2 = model.infer_vector(can)\n",
    "    similarityScore = spatial.distance.cosine(vec1, vec2)\n",
    "    cosineScore.append(similarityScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38418057544922535\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cosineScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
